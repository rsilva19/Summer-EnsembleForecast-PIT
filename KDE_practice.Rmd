---
title: "KDE Practice"
author: "Rebecca Silva"
date: "June 25, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(magrittr)
```

## R Markdown
Example 1 & 2: from http://users.stat.umn.edu/~helwig/notes/den-Notes.pdf pg 28
```{r}
set.seed(1)
x<- runif(20)
hist(x, freq=F)
kde<- density(x) #default kernal=guassian
plot(kde)
kde<- density(x,kernel ='epanechnikov')
plot (kde)
kde<- density(x, kernel = 'rectangular')
plot(kde)
```
Example 2
```{r}
set.seed(1)
x<- rnorm(20,0,1)
hist(x, freq=F)
kde<- density(x) #default kernal=guassian
plot(kde)
kde<- density(x,kernel ='epanechnikov')
plot (kde)
kde<- density(x, kernel = 'rectangular')
plot(kde)
```
Practice with normal dist and CDF
```{r}
library(spatstat) 

#spatial point pattern analysis, model-fitting, simulation, tests package
library(KernSmooth)
N<- 10000 
x<- rnorm(N,0,1)
hist(x, freq=F)
d<- density(x, bw= 'nrd0') #default kernel=guassian, default bw
plot(d)


#CDF() take a kernel estimate of a probability density and computes corresponding cumulative dist fn
cdc<- CDF(d)
cdc(.5)
plot(cdc)
```




#Generating from continuous norm 
Trying kde approach. Sampling from Normal, using density() to get pdf and CDF() to get corresponding cdf. Get PIT vals by pluging in each generated sample into cdf. 
```{r}
bin_iter_size <- .1
n<- 10000
X <- rnorm(n,0,1)

kde<- density(X, kernel='gaussian')
cdf<- CDF(kde)
plot(cdf, xlim=c(-4,4))

#plug each observed into cdf
pit_v<- c()
for(d in X){
  pit_v<- c(pit_v, cdf(d))
}
hist(pit_v, freq=F)
print (ks.test(runif(n,0,1),pit_v))

```
#VS.
##Old Method with empirical cdf (code from my PIT practice Rmd.) 
```{r}
values <- c()
for (i in seq(-5, 5, bin_iter_size)){
  values <- c(values, pnorm(i+bin_iter_size,0,1)-pnorm(i,0,1))
}

empirical_cdf <- function(q, values){  
  bin_index <- 1
  for (d in seq(-5,5,bin_iter_size)){  
    if (q < d){
      break
    }
    bin_index <- bin_index + 1
  }
  return (cumsum(values)[bin_index])  
}

pit<-c()
for(d in discreteX){
  pit<- c(pit,empirical_cdf(d, values))
}

hist(pit, freq=F)

#########################################

#Comparing the 2 side-by-side: (KDE approach, empirical cdf)
par(mfrow=c(1,2))
hist(pit_v, freq=F, main= 'Histogram using KDE') #from kde cdf drawing from continuous norm
hist(pit, freq=F, main= 'Histogram using\n empirical CDF') #from empirical cdf of discrete w bw=.1
set.seed(5)
print (ks.test(runif(n,0,1),pit_v)) # pval~ .8
print (ks.test(runif(n,0,1),pit))   # pval~ .001 
#(dif seed produces dif pvals but kde approach always >.05, other always <.05)
```

Instead of getting KDE from densities, I get kde from data that is sampled from bins with those densities. Casey thought this would be a better idea since estimates are built up around data not densities. Here, I first construct pdf using density() by using sample() to generate values -5 to 5 with respective prob. from cdf of Normal. 
```{r}
bin_iter_size <- .1
n<- 10000

values <- c()
for (i in seq(-5, 5, bin_iter_size)){
  values <- c(values, pnorm(i+bin_iter_size,0,1)-pnorm(i,0,1))
}
plot(values)

sampleDist <-  function(values,n ) { 
    dist<- sample(x =seq(-5,5,bin_iter_size), n, replace = T, prob = values) 
    return(dist)
}
X<- sampleDist(values, n)

par(mfrow=c(1,2))
pdf<- density(X, kernel='gaussian')
plot(pdf)
cdf<- CDF(pdf) 
plot(cdf, xlim= c(-5,5))

pit<-c()
for(d in X){
  pit<- c(pit,cdf(d))   
}

hist(pit, freq=F) #not as good as when we just generate kde pdf from densities

```

Here, instead of using CDF() function, we make a cdf function that uses the formula for integral of pdf using gaussian dist.: 
f̂ (x)=1/n∑fi(x) is your KDE at x, then F̂ (x)=1/n∑iFi(x) from https://stats.stackexchange.com/questions/296285/find-cdf-from-an-estimated-pdf-estimated-by-kde
So each pit values in the sum of output guassian cdfs centered at observed value?
Bandwith of .1 seems best. Large bandwiths oversmooth and smaller make it 'too precise'.  
```{r}
bin_iter_size<- .1
n<-1000
set.seed(2)
values <- c()
for (i in seq(-5, 5, bin_iter_size)){
  values <- c(values, pnorm(i+bin_iter_size,0,1)-pnorm(i,0,1))
}

#plot(values)

sampleDist <-  function(values,n ) { 
    dist<- sample(x =seq(-5,5,bin_iter_size), n, replace = T, prob = values) 
    return(dist)
}
X<- sampleDist(values, n)
hist(X, freq=F)


kernel_density_cdf <- function(x, data){
  bandwith <- .1
  cdf_density <- 0 
  for (d in data){
    cdf_density <- cdf_density + pnorm(x,d,bandwith)
    }
  return (cdf_density/length(data))
}

true<- rnorm(n,0,1)

pit<-c()
for(d in true){
  pit<- c(pit,kernel_density_cdf(d, X))
}

hist(pit, freq=F)
ks.test(pit, runif(n,0,1))

#This approach seems reasonable but depending on seed, pval is sometimes <.05. When n increases pvalue decreases as well. 
#Seems like CDF() appraoch gives best results for normal. 
```
Here, I try first making kde then sampling from that and still using rnorm to get true observations: pit is bell-shaped so will not move forward sampling from kde pdf. 
```{r}
bin_iter_size<- .1
n<-10000

values <- c()
for (i in seq(-5, 5, bin_iter_size)){
  values <- c(values, pnorm(i+bin_iter_size,0,1)-pnorm(i,0,1))
}
sum(values)

s<- seq(-5,5,bin_iter_size)


kde<- density(s, kernel="gaussian", weights= values, n=512)
#plot(kde)



#sample from kde
bw<- kde$bw
sample<- rnorm(n,0,1)
x<- c()
for (d in sample){
  x<- c(x, rnorm(1,d, bw))
}
hist(x, freq=F)


#function
kernel_density_cdf <- function(x, data){
  bandwith <- bw 
  cdf_density <- 0 
  for (d in data){
    cdf_density <- cdf_density + pnorm(x,d,bandwith)
    }
  return (cdf_density/length(data))
}

true<- rnorm(n,0,1)

pit<-c()
for(d in true){
  pit<- c(pit,kernel_density_cdf(d, x))
}

hist(pit, freq=F)
ks.test(pit, runif(n,0,1))
```